{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDwKwuoJM2AzxmPCoqhXkJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PaatriickC/CSCI-166-Project/blob/main/CSCI166FinalProject.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Space Invaders DQN + Double DQN Notebook"
      ],
      "metadata": {
        "id": "SoDtw9AaoIfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config / Imports"
      ],
      "metadata": {
        "id": "2B1XBpIzoPRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === CONFIG ===\n",
        "!pip install gymnasium[atari,accept-rom-license] autorom stable-baselines3 --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5oBRNtpfpqU1",
        "outputId": "58a263f6-b37c-4a6c-fad9-70eeb97205e4"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!AutoROM --accept-license"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Om3XozYp1Wf",
        "outputId": "ade06cad-4499-4a58-bd75-997419c99ad6"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AutoROM will download the Atari 2600 ROMs.\n",
            "They will be installed to:\n",
            "\t/usr/local/lib/python3.12/dist-packages/AutoROM/roms\n",
            "\n",
            "Existing ROMs will be overwritten.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python imports\n",
        "from dataclasses import dataclass\n",
        "import argparse, time\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "import collections\n",
        "import typing as tt\n",
        "import os\n",
        "from pathlib import Path\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.tensorboard.writer import SummaryWriter"
      ],
      "metadata": {
        "id": "YpHLSjWlp4n2"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Gym + wrappers\n",
        "import ale_py\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "from stable_baselines3.common import atari_wrappers"
      ],
      "metadata": {
        "id": "g7kgCyaKqSi7"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Directories for saving\n",
        "save_dir_drive = \"/content/drive/MyDrive/PUBLIC/Models\"\n",
        "save_dir_local = \"saved_models\"\n",
        "os.makedirs(save_dir_drive, exist_ok=True)\n",
        "os.makedirs(save_dir_local, exist_ok=True)\n",
        "os.makedirs(\"videos\", exist_ok=True)"
      ],
      "metadata": {
        "id": "8er0BJAWqfYD"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fixed model + wrappers"
      ],
      "metadata": {
        "id": "sCavkp4pq0hN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === MODEL ===\n",
        "class DQN(nn.Module):\n",
        "    def __init__(self, input_shape, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "        )\n",
        "        # compute conv output size by forwarding a dummy\n",
        "        with torch.no_grad():\n",
        "            dummy = torch.zeros(1, *input_shape)\n",
        "            size = self.conv(dummy).size()[-1]\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(size, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, n_actions)\n",
        "        )\n",
        "    def forward(self, x: torch.ByteTensor):\n",
        "        # input expected as ByteTensor [0..255]\n",
        "        x = x.float() / 255.0\n",
        "        return self.fc(self.conv(x))"
      ],
      "metadata": {
        "id": "7usVE45Yq4zB"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === WRAPPERS ===\n",
        "class ImageToPyTorch(gym.ObservationWrapper):\n",
        "    def __init__(self, env):\n",
        "        super().__init__(env)\n",
        "        obs = self.observation_space\n",
        "        assert isinstance(obs, gym.spaces.Box)\n",
        "        assert len(obs.shape) == 3\n",
        "        new_shape = (obs.shape[-1], obs.shape[0], obs.shape[1])\n",
        "        self.observation_space = gym.spaces.Box(\n",
        "            low=obs.low.min(), high=obs.high.max(),\n",
        "            shape=new_shape, dtype=obs.dtype)\n",
        "\n",
        "    def observation(self, observation):\n",
        "        return np.moveaxis(observation, 2, 0)\n",
        "\n",
        "class BufferWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, n_steps):\n",
        "        super(BufferWrapper, self).__init__(env)\n",
        "        obs = env.observation_space\n",
        "        assert isinstance(obs, spaces.Box)\n",
        "        low = np.repeat(obs.low[np.newaxis, ...], n_steps, axis=0)\n",
        "        high = np.repeat(obs.high[np.newaxis, ...], n_steps, axis=0)\n",
        "        # new shape (C*n_steps, H, W)\n",
        "        new_shape = (obs.shape[0]*n_steps, obs.shape[1], obs.shape[2])\n",
        "        self.observation_space = gym.spaces.Box(low=low.min(), high=high.max(),\n",
        "                                                shape=new_shape, dtype=obs.dtype)\n",
        "        self.buffer = collections.deque(maxlen=n_steps)\n",
        "        self.n_steps = n_steps\n",
        "\n",
        "    def reset(self, *, seed: tt.Optional[int] = None, options: tt.Optional[dict[str, tt.Any]] = None):\n",
        "        # initialize buffer with zeros\n",
        "        self.buffer.clear()\n",
        "        for _ in range(self.n_steps):\n",
        "            self.buffer.append(np.zeros_like(self.env.observation_space.low))\n",
        "        obs, extras = self.env.reset(seed=seed)\n",
        "        return self.observation(obs), extras\n",
        "\n",
        "    def observation(self, observation: np.ndarray) -> np.ndarray:\n",
        "        self.buffer.append(observation)\n",
        "        stacked = np.concatenate(list(self.buffer), axis=0)\n",
        "        return stacked\n",
        "\n",
        "def make_env(env_name: str, n_steps=4, render_mode=None, clip_reward=False, noop_max=0):\n",
        "    print(f\"Creating environment {env_name} (render_mode={render_mode})\")\n",
        "    env = gym.make(env_name, render_mode=render_mode)\n",
        "    env = atari_wrappers.AtariWrapper(env, clip_reward=clip_reward, noop_max=noop_max)\n",
        "    env = ImageToPyTorch(env)  # -> (C, H, W)\n",
        "    env = BufferWrapper(env, n_steps=n_steps)\n",
        "    return env"
      ],
      "metadata": {
        "id": "IkPlO-r5sEIW"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experience Buffer & Agent"
      ],
      "metadata": {
        "id": "gJXc-4UO2C68"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EXPERIENCE & REPLAY ===\n",
        "State = np.ndarray\n",
        "Action = int\n",
        "BatchTensors = tt.Tuple[\n",
        "    torch.ByteTensor, torch.LongTensor, torch.Tensor, torch.BoolTensor, torch.ByteTensor\n",
        "]\n",
        "\n",
        "@dataclass\n",
        "class Experience:\n",
        "    state: State\n",
        "    action: Action\n",
        "    reward: float\n",
        "    done_trunc: bool\n",
        "    new_state: State\n",
        "\n",
        "class ExperienceBuffer:\n",
        "    def __init__(self, capacity: int):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def append(self, experience: Experience):\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def sample(self, batch_size: int) -> tt.List[Experience]:\n",
        "        indices = np.random.choice(len(self), batch_size, replace=False)\n",
        "        return [self.buffer[idx] for idx in indices]\n",
        "\n",
        "# === AGENT (fixed env->self.env) ===\n",
        "class Agent:\n",
        "    def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "        self.env = env\n",
        "        self.exp_buffer = exp_buffer\n",
        "        self.state: tt.Optional[np.ndarray] = None\n",
        "        self._reset()\n",
        "\n",
        "    def _reset(self):\n",
        "        self.state, _ = self.env.reset()\n",
        "        self.total_reward = 0.0\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def play_step(self, net: DQN, device: torch.device,\n",
        "                  epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "        done_reward = None\n",
        "\n",
        "        if np.random.random() < epsilon:\n",
        "            action = self.env.action_space.sample()\n",
        "        else:\n",
        "            state_v = torch.as_tensor(self.state).to(device)\n",
        "            state_v = state_v.unsqueeze(0)  # add batch dim\n",
        "            q_vals_v = net(state_v)\n",
        "            _, act_v = torch.max(q_vals_v, dim=1)\n",
        "            action = int(act_v.item())\n",
        "\n",
        "        # do step in the environment\n",
        "        new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "        self.total_reward += float(reward)\n",
        "\n",
        "        exp = Experience(\n",
        "            state=self.state, action=action, reward=float(reward),\n",
        "            done_trunc=is_done or is_tr, new_state=new_state\n",
        "        )\n",
        "        self.exp_buffer.append(exp)\n",
        "        self.state = new_state\n",
        "        if is_done or is_tr:\n",
        "            done_reward = self.total_reward\n",
        "            self._reset()\n",
        "        return done_reward"
      ],
      "metadata": {
        "id": "Dr57AyV82Fi2"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# === AGENT (fixed env -> self.env) ===\n",
        "class Agent:\n",
        "  def __init__(self, env: gym.Env, exp_buffer: ExperienceBuffer):\n",
        "    self.env = env\n",
        "    self.exp_buffer = exp_buffer\n",
        "    self.state: tt.Optional[np.ndarray] = None\n",
        "    self._reset()\n",
        "\n",
        "  def _reset(self):\n",
        "    self.state, _ = self.env.reset()\n",
        "    self.total_reward = 0.0\n",
        "\n",
        "  @torch.no_grad()\n",
        "  def play_step(self, net: DQN, device: torch.device,\n",
        "                epsilon: float = 0.0) -> tt.Optional[float]:\n",
        "      done_reward = None\n",
        "\n",
        "      if np.random.random() < epsilon:\n",
        "        action = self.env.action_space.sample()\n",
        "      else:\n",
        "        state_v = torch.as_tensor(self.state).to(device)\n",
        "        state_v = state_v.unsqueeze(0) # add batch dim\n",
        "        q_vals_v = net(state_v)\n",
        "        _, act_v = torch.max(q_vals_v, dim=1)\n",
        "        action = int(act_v.item())\n",
        "\n",
        "      # do step in the environment\n",
        "      new_state, reward, is_done, is_tr, _ = self.env.step(action)\n",
        "      self.total_reward += float(reward)\n",
        "\n",
        "      exp = Experience(\n",
        "          state=self.state, action=action, reward=float(reward),\n",
        "          done_trunc=is_done or is_tr, new_state=new_state\n",
        "      )\n",
        "      self.exp_buffer.append(exp)\n",
        "      self.state = new_state\n",
        "      if is_done or is_tr:\n",
        "        done_reward = self.total_reward\n",
        "        self._reset()\n",
        "      return done_reward"
      ],
      "metadata": {
        "id": "AFwLu1w74-iG"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loss Function"
      ],
      "metadata": {
        "id": "YRFuwDyK7Ckk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === LOSS: supports baseline DQN and Double DQN via `double_dqn` flag ===\n",
        "def batch_to_tensors(batch: tt.List[Experience], device: torch.device) -> BatchTensors:\n",
        "    states, actions, rewards, dones, new_state = [], [], [], [], []\n",
        "    for e in batch:\n",
        "        states.append(e.state)\n",
        "        actions.append(e.action)\n",
        "        rewards.append(e.reward)\n",
        "        dones.append(e.done_trunc)\n",
        "        new_state.append(e.new_state)\n",
        "    states_t = torch.as_tensor(np.asarray(states))\n",
        "    actions_t = torch.LongTensor(actions)\n",
        "    rewards_t = torch.FloatTensor(rewards)\n",
        "    dones_t = torch.BoolTensor(dones)\n",
        "    new_states_t = torch.as_tensor(np.asarray(new_state))\n",
        "    return states_t.to(device), actions_t.to(device), rewards_t.to(device), \\\n",
        "           dones_t.to(device),  new_states_t.to(device)\n",
        "\n",
        "def calc_loss(batch: tt.List[Experience], net: DQN, tgt_net: DQN,\n",
        "              device: torch.device, gamma=0.99, double_dqn: bool=False) -> torch.Tensor:\n",
        "    states_t, actions_t, rewards_t, dones_t, new_states_t = batch_to_tensors(batch, device)\n",
        "\n",
        "    state_action_values = net(states_t).gather(1, actions_t.unsqueeze(-1)).squeeze(-1)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if double_dqn:\n",
        "            # Double DQN: action selected by online net, value taken from target net\n",
        "            next_actions = net(new_states_t).argmax(dim=1, keepdim=True)  # (B,1)\n",
        "            next_state_values = tgt_net(new_states_t).gather(1, next_actions).squeeze(-1)\n",
        "        else:\n",
        "            # vanilla DQN\n",
        "            next_state_values = tgt_net(new_states_t).max(1)[0]\n",
        "\n",
        "        next_state_values[dones_t] = 0.0\n",
        "        next_state_values = next_state_values.detach()\n",
        "\n",
        "    expected_state_action_values = rewards_t + gamma * next_state_values\n",
        "    return nn.MSELoss()(state_action_values, expected_state_action_values)"
      ],
      "metadata": {
        "id": "G0utS6KO7FHP"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training Loop"
      ],
      "metadata": {
        "id": "W-58R2YT8lbH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === TRAINING LOOP (improved logging + checkpointing + Double DQN toggle) ===\n",
        "# Hyperparams (you can tweak)\n",
        "DEFAULT_ENV_NAME = \"ALE/Pong-v5\"\n",
        "env_name = DEFAULT_ENV_NAME\n",
        "MEAN_REWARD_BOUND = 19\n",
        "\n",
        "GAMMA = 0.99\n",
        "BATCH_SIZE = 32\n",
        "REPLAY_SIZE = 100000\n",
        "LEARNING_RATE = 1e-4\n",
        "SYNC_TARGET_FRAMES = 10000\n",
        "REPLAY_START_SIZE = 10000\n",
        "\n",
        "EPSILON_DECAY_LAST_FRAME = 1_000_000\n",
        "EPSILON_START = 1.0\n",
        "EPSILON_FINAL = 0.01\n",
        "\n",
        "# Quick test config (if you want very short local test)\n",
        "# REPLAY_SIZE = 5000\n",
        "# REPLAY_START_SIZE = 1000\n",
        "# EPSILON_DECAY_LAST_FRAME = 10_000\n",
        "# SYNC_TARGET_FRAMES = 500\n",
        "\n",
        "# Variant switch\n",
        "USE_DOUBLE_DQN = True   # <-- flip this to False to run vanilla DQN\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "env = make_env(env_name, n_steps=4, render_mode=None)\n",
        "net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net = DQN(env.observation_space.shape, env.action_space.n).to(device)\n",
        "tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "writer = SummaryWriter(comment=f\"-{env_name}-{'DDQN' if USE_DOUBLE_DQN else 'DQN'}\")\n",
        "\n",
        "buffer = ExperienceBuffer(REPLAY_SIZE)\n",
        "agent = Agent(env, buffer)\n",
        "epsilon = EPSILON_START\n",
        "\n",
        "optimizer = optim.Adam(net.parameters(), lr=LEARNING_RATE)\n",
        "total_rewards = []\n",
        "frame_idx = 0\n",
        "ts_frame = 0\n",
        "ts = time.time()\n",
        "best_m_reward = None\n",
        "loss_history = []\n",
        "\n",
        "start_time = time.time()\n",
        "while True:\n",
        "    frame_idx += 1\n",
        "    epsilon = max(EPSILON_FINAL, EPSILON_START - frame_idx / EPSILON_DECAY_LAST_FRAME)\n",
        "\n",
        "    reward = agent.play_step(net, device, epsilon)\n",
        "    if reward is not None:\n",
        "        total_rewards.append(reward)\n",
        "        speed = (frame_idx - ts_frame) / (time.time() - ts + 1e-8)\n",
        "        elapsed = time.time() - start_time\n",
        "        ts_frame = frame_idx\n",
        "        ts = time.time()\n",
        "        m_reward = np.mean(total_rewards[-100:])\n",
        "        writer.add_scalar(\"epsilon\", epsilon, frame_idx)\n",
        "        writer.add_scalar(\"speed\", speed, frame_idx)\n",
        "        writer.add_scalar(\"reward_100\", m_reward, frame_idx)\n",
        "        writer.add_scalar(\"reward\", reward, frame_idx)\n",
        "\n",
        "        # print and maybe save model\n",
        "        if best_m_reward is None or m_reward > best_m_reward + 0.5:\n",
        "            print(f\"{frame_idx}: done {len(total_rewards)} games, reward {m_reward:.3f}, \"\n",
        "                  f\"eps {epsilon:.3f}, speed {speed:.2f} f/s, time {elapsed/60:.1f} min\")\n",
        "            timestamp = datetime.now().strftime(\"%Y%m%d-%H%M\")\n",
        "            safe_env_name = env_name.replace(\"/\", \"_\")\n",
        "            model_filename = f\"{safe_env_name}-best_{int(m_reward)}-{timestamp}.dat\"\n",
        "            torch.save(net.state_dict(), os.path.join(save_dir_local, model_filename))\n",
        "            torch.save(net.state_dict(), os.path.join(save_dir_drive, model_filename))\n",
        "            best_m_reward = m_reward\n",
        "\n",
        "        if m_reward > MEAN_REWARD_BOUND:\n",
        "            print(\"Solved in %d frames!\" % frame_idx)\n",
        "            break\n",
        "\n",
        "    if len(buffer) < REPLAY_START_SIZE:\n",
        "        continue\n",
        "\n",
        "    if frame_idx % SYNC_TARGET_FRAMES == 0:\n",
        "        tgt_net.load_state_dict(net.state_dict())\n",
        "\n",
        "    # optimize\n",
        "    optimizer.zero_grad()\n",
        "    batch = buffer.sample(BATCH_SIZE)\n",
        "    loss_t = calc_loss(batch, net, tgt_net, device, gamma=GAMMA, double_dqn=USE_DOUBLE_DQN)\n",
        "    loss_t.backward()\n",
        "    # optional grad clip\n",
        "    nn.utils.clip_grad_norm_(net.parameters(), 10.0)\n",
        "    optimizer.step()\n",
        "    loss_history.append(float(loss_t.detach().cpu().numpy()))\n",
        "\n",
        "# close\n",
        "env.close()\n",
        "writer.close()\n",
        "\n",
        "# Save training artifacts\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(total_rewards, label='episodic reward')\n",
        "if len(total_rewards) >= 100:\n",
        "    import numpy as np\n",
        "    rol = np.convolve(total_rewards, np.ones(100)/100, mode='valid')\n",
        "    plt.plot(range(99,99+len(rol)), rol, label='100-ep rolling')\n",
        "plt.xlabel(\"Episodes\")\n",
        "plt.ylabel(\"Reward\")\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.savefig(\"results_learning_curve.png\", dpi=150)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx-yUtCm8nKY",
        "outputId": "a1cb8ed7-6118-4ff8-fb0f-80a18405927b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating environment ALE/Pong-v5 (render_mode=None)\n",
            "211: done 1 games, reward -21.000, eps 1.000, speed 340.78 f/s, time 0.0 min\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation function & Video Recording"
      ],
      "metadata": {
        "id": "aKl7KELa8p0W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# === EVALUATION & VIDEO RECORDING ===\n",
        "from gymnasium.wrappers import RecordVideo\n",
        "\n",
        "def evaluate_and_record(model, env_name, model_path=None, out_path=\"videos/eval.mp4\",\n",
        "                        steps=1000, greedy=True, n_steps=4):\n",
        "    # create env with rgb_array to record frames\n",
        "    eval_env = make_env(env_name, n_steps=n_steps, render_mode=\"rgb_array\")\n",
        "    # gymnasium RecordVideo requires the older wrapper name; we use stable method below:\n",
        "    eval_env = RecordVideo(eval_env, video_folder=\"videos\", name_prefix=\"eval\")\n",
        "    state, _ = eval_env.reset()\n",
        "    total_rw = 0.0\n",
        "    for t in range(steps):\n",
        "        state_v = torch.as_tensor(state).to(device).unsqueeze(0)\n",
        "        q = model(state_v)\n",
        "        if greedy:\n",
        "            action = int(q.argmax(dim=1).item())\n",
        "        else:\n",
        "            action = eval_env.action_space.sample()\n",
        "        state, reward, done, trunc, _ = eval_env.step(action)\n",
        "        total_rw += float(reward)\n",
        "        if done or trunc:\n",
        "            state, _ = eval_env.reset()\n",
        "            break\n",
        "    eval_env.close()\n",
        "    print(\"Eval total reward:\", total_rw)\n",
        "\n",
        "# Usage example (after you have some saved checkpoint):\n",
        "# net.load_state_dict(torch.load(\"saved_models/your_model.dat\"))\n",
        "# evaluate_and_record(net, env_name, steps=600, greedy=False)  # early randomish\n",
        "# evaluate_and_record(net, env_name, steps=600, greedy=True)   # later learned\n"
      ],
      "metadata": {
        "id": "n-4BnvAE8sTT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}